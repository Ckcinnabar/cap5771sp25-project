# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sIwnw4mSmiIB7rb-khZX06MQfp4lEzDV

# Kuan Chen Chen Milestone 2

## import library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from dateutil.relativedelta import relativedelta
import os
import json
import requests
import io
import seaborn as sns
import re
import tensorflow as tf
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, KFold
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer
from joblib import dump, load





from matplotlib import pyplot as plt
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go

"""## Dataset manipulate

### Scaler & Label

#### Overall scaler
"""

raw_data = pd.read_csv("all_data.csv")
raw_data.describe(),raw_data.info()

raw_data['enter_pro']=raw_data['Age']-raw_data['years_pro']
raw_data['bmi'] =  raw_data['weight_kg']/((raw_data['height_cm']/100)**2)
raw_data

data_scaled = raw_data.copy()
mm_scaler = MinMaxScaler()
ss_scaler = StandardScaler()

for sport in ["NFL", "NBA","FIFA"]:
    sport_mask = data_scaled["sport"] == sport
    sport_indices = data_scaled[sport_mask].index
    sport_overall = data_scaled.loc[sport_mask, "overall"].values.reshape(-1, 1)

    minmax = mm_scaler.fit_transform(sport_overall)
    data_scaled.loc[sport_indices, "overall_minmax"] = minmax.flatten()

    standard = ss_scaler.fit_transform(sport_overall)
    data_scaled.loc[sport_indices, "overall_standard"] = standard.flatten()


data_scaled

"""#### Data label"""

for sport in data_scaled["sport"].unique():
    positions = data_scaled[data_scaled["sport"] == sport]["position"].unique()
    print(f"\n{sport} have {len(positions)} positions:")
    print(positions)

unique_positions = data_scaled["position"].unique()
position_count = len(unique_positions)
position_count

def transform_encoding(data,column, nfl_map, nba_map, new_column_name):

    data_simplified = data.copy()
    def apply_position_rule(row):
        if row['sport'] == 'NFL':
            return nfl_map.get(row[column], row[column])
        elif row['sport'] == 'NBA':
            return nba_map.get(row[column], row[column])
        elif row['sport'] == 'FIFA':
            return 0
        else:
            return row['position']

    data_simplified[new_column_name] = data_simplified.apply(apply_position_rule, axis=1)
    return data_simplified

def check_encode(data,label):
    for sport in data['sport'].unique():
        positions = data[data['sport'] == sport][label].unique()
        print(f"\n{sport} have {len(positions)} positions:")
        print(positions)

nfl_18to18_label = {
        'QB': 1,
        'RB': 2,
        'FB': 3 ,
        'WR': 4,
        'TE': 5,
        'LT': 6,
        'RT': 7,
        'LG': 8,
        'RG': 9,
        'C': 10,
        'EDGE': 11,
        'IDL': 12,
        'LB': 13,
        'CB': 14,
        'S': 15,
        'K': 16,
        'P': 17,
        'LS': 18
    }

nba_5to5_lable = {
    'PF': 4,
    'PG': 1,
    'SF': 3 ,
    'SG': 2,
    'C': 5
}

data_label = transform_encoding(data_scaled,"position", nfl_18to18_label, nba_5to5_lable , "position_label")
data_label

check_encode(data_label,"position_label")

"""#### Position decrease"""

nfl_18to8_map = {
        'QB': 'QB',
        'RB': 'RB',
        'FB': 'RB',
        'WR': 'Receiver',
        'TE': 'Receiver',
        'LT': 'OL',
        'RT': 'OL',
        'LG': 'OL',
        'RG': 'OL',
        'C': 'OL',
        'EDGE': 'DL',
        'IDL': 'DL',
        'LB': 'LB',
        'CB': 'DB',
        'S': 'DB',
        'K': 'Specialist',
        'P': 'Specialist',
        'LS': 'Specialist'S
    }

nba_5to3_map = {
    'PF':'F',
    'PG':'G',
    'SF':'F',
    'SG':'G',
    'C':'C'
}

data_encode = transform_encoding(data_label,"position",nfl_18to8_map,nba_5to3_map,"position_simplified")

check_encode(data_encode,'position_simplified' )

data_encode

nfl_8to8_label = {
    'QB': 1,
    'Receiver': 2,
    'DL': 3,
    'OL': 4,
    'DB': 5,
    'LB': 6,
    'RB': 7,
    'Specialist': 8
    }

nba_3to3_label = {
    'F':2,
    'G':1,
    'C':3
}

df = transform_encoding(data_encode,'position_simplified',nfl_8to8_label,nba_3to3_label,"position_encode_simplified")
check_encode(df,"position_encode_simplified")

"""### Data split"""

df_no_years = df.drop(labels="years_pro",  axis=1)
df_no_years

df_pro_years = df[df["sport"].isin(["NFL", "NBA"])]
df_pro_years

nba_year = df_pro_years[ df_pro_years["sport"] == "NBA"]
nfl_year = df_pro_years[ df_pro_years["sport"] == "NFL"]
nba = df_no_years [ df_no_years ["sport"] == "NBA"]
nfl =df_no_years[ df_no_years["sport"] == "NFL"]
fifa = df_no_years [ df_no_years ["sport"] == "FIFA"]

dataset = [
    (nba_year, "nba_year"),
    (nfl_year, "nfl_year"),
    (nba, "nba"),
    (nfl, "nfl"),
    #(fifa, "fifa")
]

"""## EDA"""

def corr_matrix_plt(df,feature):
    corr_matrix = df[feature].corr()
    for col in feature:
        total_abs_corr = corr_matrix[col].abs().sum() - 1
        print(f"{col}: {total_abs_corr:.2f}")
    plt.figure(figsize=(12, 10))
    sns.heatmap(df[feature].corr(), annot=True, cmap='coolwarm', center=0)
    plt.title('correlation between feature data')
    plt.show()

"""### nba"""

corr_matrix_plt(nba_year,["overall","height_cm", "weight_kg","position_label", "Age", "years_pro","bmi","enter_pro","APY"])

"""### nfl"""

corr_matrix_plt(nfl_year,["overall","height_cm", "weight_kg","position_label", "Age", "years_pro","bmi","enter_pro","APY"])

"""### fifa"""

corr_matrix_plt(fifa,["overall","height_cm", "weight_kg", "Age","bmi","APY"])

"""## Linear regression without position(hypothesis)"""

def APY_with_overall_linear_regression_model(dataset, overall_type="overall", years=None):

    dataset_name = dataset.__name__ if hasattr(dataset, "__name__") else str(dataset)
    has_years_pro = "years_pro" in dataset.columns

    if has_years_pro:
        x = dataset[[overall_type, "height_cm", "weight_kg", "Age", "years_pro","bmi","enter_pro"]]
    else:
        x = dataset[[overall_type, "height_cm", "weight_kg", "Age","bmi","enter_pro"]]

    y = dataset["APY"]
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)

    lin_model = LinearRegression()
    lin_model.fit(x_train, y_train)

    y_pred = lin_model.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model Results(with overall):")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

def APY_without_overall_linear_regression_model(dataset, years=None):

    dataset_name = dataset.__name__ if hasattr(dataset, "__name__") else str(dataset)
    has_years_pro = "years_pro" in dataset.columns

    if has_years_pro:
        x = dataset[[overall_type, "height_cm", "weight_kg", "Age", "years_pro","bmi","enter_pro"]]
    else:
        x = dataset[[overall_type, "height_cm", "weight_kg", "Age","bmi","enter_pro"]]

    y = dataset["APY"]
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)

    lin_model = LinearRegression()
    lin_model.fit(x_train, y_train)

    y_pred = lin_model.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model Results(without overall):")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

model_try = [APY_with_overall_linear_regression_model, APY_without_overall_linear_regression_model]

"""## Building model

### Linear models

#### basic
"""

def linear_regression_model(dataset,features, overall_type="overall", years=None):
    dataset_name = dataset.__name__ if hasattr(dataset, "__name__") else str(dataset)
    has_years_pro = "years_pro" in dataset.columns

    if has_years_pro:
        features_to_use = features.copy()
    else:
        features_to_use = [feat for feat in features if feat not in ["years_pro", "enter_pro"]]

    x = dataset[features_to_use]

    y = dataset[overall_type]
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)

    lin_model = LinearRegression()
    lin_model.fit(x_train, y_train)

    y_pred = lin_model.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model Results:")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

"""#### ridge"""

def ridge_model(dataset,features, overall_type="overall", years=None):
    dataset_name = dataset.__name__ if hasattr(dataset, "____name__") else str(dataset)
    has_years_pro = "years_pro" in dataset.columns

    if has_years_pro:
        features_to_use = features.copy()
    else:
        features_to_use = [feat for feat in features if feat not in ["years_pro", "enter_pro"]]

    x = dataset[features_to_use]

    y = dataset[overall_type]
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)

    param_grid = {
        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],
        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
    }
    ridge = Ridge(random_state=2025)

    grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(x_train, y_train)
    best_model = grid_search.best_estimator_

    y_pred = best_model.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model Results (with tuned hyperparameters):")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

"""#### lasso"""

def lasso_model(dataset,features, overall_type="overall", years=None):
    dataset_name = dataset.__name__ if hasattr(dataset, "____name__") else str(dataset)
    has_years_pro = "years_pro" in dataset.columns

    if has_years_pro:
        features_to_use = features.copy()
    else:
        features_to_use = [feat for feat in features if feat not in ["years_pro", "enter_pro"]]

    x = dataset[features_to_use]

    y = dataset[overall_type]
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)

    param_grid = {
        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],
        'selection': ['cyclic', 'random']
    }
    lasso = Lasso(random_state=2025, max_iter=10000)

    grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(x_train, y_train)
    best_model = grid_search.best_estimator_

    y_pred = best_model.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model Results (with tuned hyperparameters):")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

"""### Nonlinear Models

#### RandomForestRegressor
"""

def randomforest_regression_model(dataset, features,overall_type="overall", years=None):
    dataset_name = dataset.__name__ if hasattr(dataset, "__name__") else str(dataset)
    has_years_pro = "years_pro" in dataset.columns

    if has_years_pro:
        features_to_use = features.copy()
    else:
        features_to_use = [feat for feat in features if feat not in ["years_pro", "enter_pro"]]

    x = dataset[features_to_use]
    y = dataset[overall_type]
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)

    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 5, 10, 15],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    rf = RandomForestRegressor(random_state=2025)
    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='r2',n_jobs=-1)
    grid_search.fit(x_train, y_train)
    best_rf = grid_search.best_estimator_
    y_pred = best_rf.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model Results (with tuned hyperparameters):")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

"""#### GradientBoosting"""

def GradientBoostingRegressor_model(dataset,features, overall_type="overall", years=None):
    dataset_name = dataset.__name__ if hasattr(dataset, "__name__") else str(dataset)
    has_years_pro = "years_pro" in dataset.columns

    if has_years_pro:
        features_to_use = features.copy()
    else:
        features_to_use = [feat for feat in features if feat not in ["years_pro", "enter_pro"]]

    x = dataset[features_to_use]
    y = dataset[overall_type]
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)

    param_grid = {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'subsample': [0.8, 0.9, 1.0]
    }

    gb_model = GradientBoostingRegressor(random_state=2025)
    grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='r2', n_jobs=-1)


    grid_search.fit(x_train, y_train)
    best_rf = grid_search.best_estimator_
    y_pred = best_rf.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model Results (with tuned hyperparameters):")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

"""## Runing model"""

def run_model(datasets_with_names, features, models):
    results = []
    if not any(isinstance(feat, list) for feat in features):
        features = [features]

    for dataset, dataset_name in datasets_with_names:
        for feature_set in features:
            for model_func in models:
                print(f"Running {model_func.__name__} on {dataset_name} with features {feature_set}")
                try:

                    model_result = model_func(dataset, feature_set)
                    results.append({
                        "dataset": dataset_name,
                        "model": model_func.__name__,
                        "features": feature_set,
                        "results": model_result
                    })
                    print("-" * 50)
                except Exception as e:
                    print(f"Error running {model_func.__name__} on {dataset_name} with features {feature_set}: {e}")

"""### checking hypothesis"""

def run_model_test(datasets_with_names, models):
    results = []

    for dataset, dataset_name in datasets_with_names:

        for model_func in models:
            print(f"Running {model_func.__name__} on {dataset_name}")
            try:
                model_result = model_func(dataset)
                results.append({
                    "dataset": dataset_name,
                    "model": model_func.__name__,
                    "results": model_result
                })
                print("-" * 50)
            except Exception as e:
                print(f"Error running {model_func.__name__} on {dataset_name}: {e}")

run_model_test(dataset, model_try)

def run_model_check(datasets_with_names, model_func, **model_kwargs):
    results = []
    for dataset, dataset_name in datasets_with_names:
        print(f"Running {model_func.__name__} on {dataset_name}")

        try:
            model_result = model_func(dataset, **model_kwargs)
            results.append({
                'dataset': dataset_name,
                'model': model_func.__name__,
                'results': model_result
            })

            print("-" * 50)
        except Exception as e:
            print(f"Error running {model_func.__name__} on {dataset_name}: {e}")

run_model_check(dataset, APY_with_overall_linear_regression_model, overall_type="overall_standard")

run_model_check(dataset, APY_with_overall_linear_regression_model, overall_type="overall_minmax")

"""### performance

#### NBA
"""

model=[linear_regression_model,ridge_model,lasso_model,randomforest_regression_model,GradientBoostingRegressor_model]

dataset_nba = [
    (nba_year, "nba_year"),
    (nba, "nba")]


features = [["height_cm", "weight_kg", "Age", "years_pro","bmi","enter_pro"],
            ["height_cm", "weight_kg","position_label", "Age", "years_pro","bmi","enter_pro"],
            ["height_cm", "weight_kg","position_encode_simplified", "Age", "years_pro","bmi","enter_pro"]
           ]

run_model(dataset_nba,features,model)

nba_model = GradientBoostingRegressor(
    learning_rate= 0.01,
    max_depth= 7,
    min_samples_leaf=2,
    min_samples_split= 5,
    n_estimators= 200,
    subsample= 0.8,
    random_state=2025
)


x = nba_year[['height_cm', 'weight_kg', 'position_label', 'Age', 'years_pro', 'bmi', 'enter_pro'] ]
y = nba_year['overall']
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)


nba_model.fit(x_train, y_train)
nba_model_data = {
    'model': nba_model,
    'features': ['height_cm', 'weight_kg', 'position_label', 'Age', 'years_pro', 'bmi', 'enter_pro']
}
dump(nba_model, 'nba_model.joblib')

"""### FIFA"""

dataset_fifa = [(fifa, "fifa")]
features_fifa = [["height_cm", "weight_kg", "Age", "years_pro","bmi","enter_pro"]]

run_model(dataset_fifa ,features,model)

fifa

fifa_model = GradientBoostingRegressor(
    learning_rate= 0.1,
    max_depth= 3,
    min_samples_leaf=1,
    min_samples_split= 2,
    n_estimators= 50,
    subsample= 1.0,
    random_state=2025
)


x = fifa[["height_cm", "weight_kg", "Age","bmi"] ]
y = fifa['overall']
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)


fifa_model.fit(x_train, y_train)
fifa_model_data = {
    'model': fifa_model,
    'features': ["height_cm", "weight_kg", "Age","bmi"]
}
dump(fifa_model, 'fifa_model.joblib')

"""### NFL"""

dataset_nfl = [
    (nfl_year, "nfl_year"),
    (nfl, "nba")]

run_model(dataset_nfl,features,model)

nfl_year

nfl_model = RandomForestRegressor(
    max_depth=5,
    min_samples_leaf=4,
    min_samples_split=10,
    n_estimators=200,
    random_state=42
)


x = nfl_year[['height_cm', 'weight_kg', 'position_label', 'Age', 'years_pro', 'bmi', 'enter_pro'] ]
y = nfl_year['overall']
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2025, test_size=0.2)


nfl_model.fit(x_train, y_train)
nfl_model_data = {
    'model': nfl_model,
    'features': ['height_cm', 'weight_kg', 'position_label', 'Age', 'years_pro', 'bmi', 'enter_pro']
}
dump(nfl_model_data, 'nfl_model.joblib')